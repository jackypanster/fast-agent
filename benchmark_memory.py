#!/usr/bin/env python3
"""
Platform Agent MemoryåŠŸèƒ½æ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•ç›®æ ‡ï¼š
1. æµ‹é‡memoryå¯ç”¨å‰åçš„å“åº”æ—¶é—´å·®å¼‚
2. ç»Ÿè®¡å·¥å…·è°ƒç”¨æ¬¡æ•°çš„å‡å°‘æƒ…å†µ
3. è¯„ä¼°memoryå¯¹æ•´ä½“æ€§èƒ½çš„å½±å“
4. ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    uv run benchmark_memory.py
    uv run benchmark_memory.py --detailed  # è¯¦ç»†æ¨¡å¼
    uv run benchmark_memory.py --export-csv # å¯¼å‡ºCSVæ ¼å¼
"""

import os
import sys
import time
import json
import argparse
import statistics
import subprocess
from datetime import datetime
from typing import Dict, List, Tuple, Any
from pathlib import Path


class MemoryBenchmark:
    """MemoryåŠŸèƒ½æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, detailed: bool = False):
        self.detailed = detailed
        self.results = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "platform": sys.platform,
                "python_version": sys.version,
            },
            "benchmarks": [],
            "summary": {}
        }
        
        # æµ‹è¯•ç”¨ä¾‹é›†
        self.test_cases = [
            {
                "name": "simple_cluster_query",
                "query": "show me all k8s clusters",
                "expected_tools": ["get_cluster_info"]
            },
            {
                "name": "cluster_detail_query", 
                "query": "which cluster has the most pods?",
                "expected_tools": []  # åº”è¯¥ä»memoryè·å–
            },
            {
                "name": "follow_up_query",
                "query": "what about the production environment?",
                "expected_tools": []  # åº”è¯¥ä»memoryè·å–
            },
            {
                "name": "complex_analysis",
                "query": "analyze the health status of all clusters and recommend actions",
                "expected_tools": ["get_cluster_info"]  # å¯èƒ½éœ€è¦å·¥å…·è°ƒç”¨
            }
        ]
    
    def measure_response_time(self, query: str, with_memory: bool = True) -> Dict[str, Any]:
        """æµ‹é‡å•ä¸ªæŸ¥è¯¢çš„å“åº”æ—¶é—´å’Œå·¥å…·è°ƒç”¨æƒ…å†µ"""
        
        # å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
        env_vars = os.environ.copy()
        if with_memory:
            env_vars["CREWAI_STORAGE_DIR"] = "./crew_memory"
        else:
            # ç¦ç”¨memoryï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
            env_vars["CREWAI_STORAGE_DIR"] = "/tmp/no_memory"
        
        # æ„é€ æµ‹è¯•è„šæœ¬
        test_script = f"""
import sys
import time
import os
sys.path.insert(0, 'src')

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ.update({repr(dict(env_vars))})

start_time = time.time()
tool_calls = 0

try:
    from ops_crew.crew import run_crew
    
    # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨è®¡æ•°ï¼ˆéœ€è¦åœ¨å®é™…å®ç°ä¸­æ·»åŠ ï¼‰
    original_run_crew = run_crew
    
    def counting_run_crew(user_input):
        global tool_calls
        # è¿™é‡Œéœ€è¦å®é™…çš„å·¥å…·è°ƒç”¨è®¡æ•°é€»è¾‘
        result = original_run_crew(user_input)
        return result
    
    result = counting_run_crew("{query}")
    end_time = time.time()
    
    print(f"BENCHMARK_RESULT:{{")
    print(f"  'response_time': {end_time - start_time:.3f},")
    print(f"  'tool_calls': {tool_calls},")
    print(f"  'success': True,")
    print(f"  'result_length': {len(str(result)) if result else 0}")
    print(f"}}")
    
except Exception as e:
    end_time = time.time()
    print(f"BENCHMARK_RESULT:{{")
    print(f"  'response_time': {end_time - start_time:.3f},")
    print(f"  'tool_calls': 0,")
    print(f"  'success': False,")
    print(f"  'error': '{str(e)}'")
    print(f"}}")
        """
        
        try:
            # è¿è¡Œæµ‹è¯•
            process = subprocess.Popen(
                ["python", "-c", test_script],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=120,  # 2åˆ†é’Ÿè¶…æ—¶
                env=env_vars
            )
            
            stdout, stderr = process.communicate()
            output = stdout.decode() + stderr.decode()
            
            # è§£æç»“æœ
            if "BENCHMARK_RESULT:" in output:
                result_line = output.split("BENCHMARK_RESULT:")[1]
                # ç®€å•çš„ç»“æœè§£æï¼ˆå®é™…åº”è¯¥æ›´robustï¼‰
                result = {
                    "response_time": 0.0,
                    "tool_calls": 0, 
                    "success": False,
                    "result_length": 0
                }
                
                # æå–å“åº”æ—¶é—´
                if "'response_time':" in result_line:
                    time_str = result_line.split("'response_time':")[1].split(",")[0].strip()
                    result["response_time"] = float(time_str)
                
                # æå–æˆåŠŸçŠ¶æ€
                result["success"] = "'success': True" in result_line
                
                return result
            else:
                return {
                    "response_time": 999.0,  # å¤±è´¥çš„æƒ…å†µ
                    "tool_calls": 0,
                    "success": False,
                    "error": "No benchmark result found in output"
                }
                
        except subprocess.TimeoutExpired:
            return {
                "response_time": 120.0,  # è¶…æ—¶
                "tool_calls": 0,
                "success": False,
                "error": "Timeout"
            }
        except Exception as e:
            return {
                "response_time": 999.0,
                "tool_calls": 0,
                "success": False,
                "error": str(e)
            }
    
    def run_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        
        print("ğŸƒâ€â™‚ï¸ Platform Agent Memoryæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        for i, test_case in enumerate(self.test_cases):
            print(f"\nğŸ“Š æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(self.test_cases)}: {test_case['name']}")
            print(f"   æŸ¥è¯¢: {test_case['query']}")
            
            # å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼
            runs = 3 if not self.detailed else 5
            
            with_memory_times = []
            without_memory_times = []
            
            for run in range(runs):
                print(f"   è¿è¡Œ {run+1}/{runs}...", end=" ")
                
                # æµ‹è¯•å¯ç”¨memoryçš„æƒ…å†µ
                result_with = self.measure_response_time(test_case["query"], with_memory=True)
                with_memory_times.append(result_with["response_time"])
                
                # æµ‹è¯•ç¦ç”¨memoryçš„æƒ…å†µï¼ˆå¦‚æœæ”¯æŒï¼‰
                result_without = self.measure_response_time(test_case["query"], with_memory=False)
                without_memory_times.append(result_without["response_time"])
                
                print("âœ“")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…ç³»ç»Ÿè´Ÿè½½
                time.sleep(1)
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            avg_with = statistics.mean(with_memory_times)
            avg_without = statistics.mean(without_memory_times)
            improvement = ((avg_without - avg_with) / avg_without * 100) if avg_without > 0 else 0
            
            benchmark_result = {
                "test_case": test_case["name"],
                "query": test_case["query"],
                "runs": runs,
                "with_memory": {
                    "avg_time": avg_with,
                    "min_time": min(with_memory_times),
                    "max_time": max(with_memory_times),
                    "times": with_memory_times
                },
                "without_memory": {
                    "avg_time": avg_without,
                    "min_time": min(without_memory_times),
                    "max_time": max(without_memory_times),
                    "times": without_memory_times
                },
                "improvement_percent": improvement
            }
            
            self.results["benchmarks"].append(benchmark_result)
            
            # è¾“å‡ºç»“æœ
            print(f"   ğŸ“ˆ ç»“æœ:")
            print(f"      å¯ç”¨Memory: {avg_with:.3f}s (å¹³å‡)")
            print(f"      ç¦ç”¨Memory: {avg_without:.3f}s (å¹³å‡)")
            if improvement > 0:
                print(f"      æ€§èƒ½æå‡: {improvement:.1f}% âœ…")
            else:
                print(f"      æ€§èƒ½å˜åŒ–: {improvement:.1f}% âš ï¸")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        self.calculate_summary()
        return self.results
    
    def calculate_summary(self):
        """è®¡ç®—æ€»ä½“ç»Ÿè®¡æ•°æ®"""
        if not self.results["benchmarks"]:
            return
        
        improvements = [b["improvement_percent"] for b in self.results["benchmarks"]]
        with_memory_times = [b["with_memory"]["avg_time"] for b in self.results["benchmarks"]]
        without_memory_times = [b["without_memory"]["avg_time"] for b in self.results["benchmarks"]]
        
        self.results["summary"] = {
            "total_test_cases": len(self.results["benchmarks"]),
            "avg_improvement_percent": statistics.mean(improvements),
            "max_improvement_percent": max(improvements),
            "min_improvement_percent": min(improvements),
            "avg_response_time_with_memory": statistics.mean(with_memory_times),
            "avg_response_time_without_memory": statistics.mean(without_memory_times),
            "performance_grade": self.get_performance_grade(statistics.mean(improvements))
        }
    
    def get_performance_grade(self, improvement_percent: float) -> str:
        """æ ¹æ®æ€§èƒ½æå‡ç™¾åˆ†æ¯”è¯„å®šç­‰çº§"""
        if improvement_percent >= 50:
            return "A+ (ä¼˜ç§€)"
        elif improvement_percent >= 30:
            return "A (è‰¯å¥½)"
        elif improvement_percent >= 10:
            return "B (åˆæ ¼)"
        elif improvement_percent >= 0:
            return "C (å¾…ä¼˜åŒ–)"
        else:
            return "D (éœ€è¦æ”¹è¿›)"
    
    def print_summary_report(self):
        """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
        summary = self.results["summary"]
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 60)
        
        print(f"æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {summary['total_test_cases']}")
        print(f"å¹³å‡æ€§èƒ½æå‡: {summary['avg_improvement_percent']:.1f}%")
        print(f"æœ€å¤§æ€§èƒ½æå‡: {summary['max_improvement_percent']:.1f}%")
        print(f"æœ€å°æ€§èƒ½æå‡: {summary['min_improvement_percent']:.1f}%")
        print(f"")
        print(f"å¯ç”¨Memoryå¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time_with_memory']:.3f}s")
        print(f"ç¦ç”¨Memoryå¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time_without_memory']:.3f}s")
        print(f"")
        print(f"æ€§èƒ½ç­‰çº§: {summary['performance_grade']}")
        
        # æ€§èƒ½å»ºè®®
        print(f"\nğŸ’¡ æ€§èƒ½å»ºè®®:")
        if summary['avg_improvement_percent'] >= 30:
            print("   âœ… MemoryåŠŸèƒ½æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå»ºè®®å¯ç”¨")
        elif summary['avg_improvement_percent'] >= 10:
            print("   âš ï¸ MemoryåŠŸèƒ½æœ‰ä¸€å®šæå‡ï¼Œå¯ä»¥è€ƒè™‘å¯ç”¨")
        else:
            print("   âŒ MemoryåŠŸèƒ½æå‡ä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    def save_report(self, filename: str = None, export_csv: bool = False):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"memory_benchmark_report_{timestamp}.json"
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(filename, "w") as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        
        # å¯¼å‡ºCSVæ ¼å¼
        if export_csv:
            csv_filename = filename.replace(".json", ".csv")
            self.export_csv(csv_filename)
            print(f"ğŸ“Š CSVæ•°æ®å·²å¯¼å‡ºåˆ°: {csv_filename}")
    
    def export_csv(self, filename: str):
        """å¯¼å‡ºCSVæ ¼å¼çš„æ•°æ®"""
        import csv
        
        with open(filename, "w", newline="") as csvfile:
            fieldnames = [
                "test_case", "query", "runs",
                "avg_time_with_memory", "avg_time_without_memory",
                "improvement_percent"
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for benchmark in self.results["benchmarks"]:
                writer.writerow({
                    "test_case": benchmark["test_case"],
                    "query": benchmark["query"],
                    "runs": benchmark["runs"],
                    "avg_time_with_memory": benchmark["with_memory"]["avg_time"],
                    "avg_time_without_memory": benchmark["without_memory"]["avg_time"],
                    "improvement_percent": benchmark["improvement_percent"]
                })


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Platform Agent Memoryæ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--detailed", action="store_true", 
                       help="è¿è¡Œè¯¦ç»†æµ‹è¯•ï¼ˆæ›´å¤šè¿è¡Œæ¬¡æ•°ï¼‰")
    parser.add_argument("--export-csv", action="store_true",
                       help="å¯¼å‡ºCSVæ ¼å¼æ•°æ®")
    parser.add_argument("--output", type=str,
                       help="æŒ‡å®šè¾“å‡ºæ–‡ä»¶å")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = MemoryBenchmark(detailed=args.detailed)
    
    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = benchmark.run_benchmark_suite()
        
        # æ‰“å°æ€»ç»“
        benchmark.print_summary_report()
        
        # ä¿å­˜æŠ¥å‘Š
        benchmark.save_report(args.output, args.export_csv)
        
        # æ ¹æ®æ€§èƒ½ç»“æœè®¾ç½®é€€å‡ºç 
        if results["summary"]["avg_improvement_percent"] >= 10:
            print("\nğŸ‰ åŸºå‡†æµ‹è¯•é€šè¿‡ï¼šMemoryåŠŸèƒ½å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼")
            sys.exit(0)
        else:
            print("\nâš ï¸ åŸºå‡†æµ‹è¯•è­¦å‘Šï¼šMemoryåŠŸèƒ½çš„æ€§èƒ½æå‡ä¸å¤Ÿæ˜æ˜¾")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ åŸºå‡†æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()